library(caret)
library(gbm)
#Funcoes utilizadas
#implmente as funcoes tpr e fpr
tpr = function(real,predito){
sum(predito == 1 & real == 1)/sum(real == 1)
}
fpr = function(real,predito){
sum(predito == 1 & real == 0)/sum(real == 0)
}
#não altere a funcao create_fold_index
create_fold_index = function(nrows, folds){
set.seed(2019)
k_index = rep(1:folds, ceiling(nrows/folds))
k_index = k_index[1:nrows]
k_index = sample(k_index)
return(k_index)
}
#Importe os dados ----
setwd("~/feagri/fa084/prova/code")
df = read.csv("../data/weather_rain_tomorrow.csv")
#Campo de dados
head(df)
nrow(df)
#converta o atributo meta para fator
df$RainTomorrow = as.factor(df$RainTomorrow)
#Divida em treino e teste
rows_test = sample(nrow(df),.3*nrow(df))
train = df[-rows_test,]
test = df[rows_test,]
#Modelagem com knn ----
#a)Estruture os dados de treino e teste para serem usados para uma modelagem
#usando a função knn() do pacote class. (0,5)
train_knn = train[,c("MinTemp","MaxTemp","Rainfall","Humidity9am","Humidity3pm")]
test_knn = test[,c("MinTemp","MaxTemp","Rainfall","Humidity9am","Humidity3pm")]
#meta
train_label = train$RainTomorrow
test_label = test$RainTomorrow
#b)Use a função knn()  para criar dois vetores de predições para o conjunto
#de teste, um com k = 5 e outro com k = 11. (0,5)
preds_knn11 =knn(train_knn,test_knn, cl = train$RainTomorrow, k = 11)
acc_knn11 = mean(preds_knn11 == test_label)
tpr_knn11 = tpr(test_label,preds_knn11)
fpr_knn11 = fpr(test_label,preds_knn11)
df = read.csv("./data/weather_rain_tomorrow.csv")
#Campo de dados
head(df)
nrow(df)
#converta o atributo meta para fator
df$RainTomorrow = as.factor(df$RainTomorrow)
#Divida em treino e teste
rows_test = sample(nrow(df),.3*nrow(df))
train = df[-rows_test,]
test = df[rows_test,]
#Modelagem com knn ----
#a)Estruture os dados de treino e teste para serem usados para uma modelagem
#usando a função knn() do pacote class. (0,5)
train_knn = train[,c("MinTemp","MaxTemp","Rainfall","Humidity9am","Humidity3pm")]
test_knn = test[,c("MinTemp","MaxTemp","Rainfall","Humidity9am","Humidity3pm")]
#meta
train_label = train$RainTomorrow
test_label = test$RainTomorrow
preds_knn11 =knn(train_knn,test_knn, cl = train$RainTomorrow, k = 11)
acc_knn11 = mean(preds_knn11 == test_label)
tpr_knn11 = tpr(test_label,preds_knn11)
fpr_knn11 = fpr(test_label,preds_knn11)
preds_knn5 = knn(train_knn,test_knn, cl = train$RainTomorrow, k = 5)
acc_knn5 =mean(preds_knn5 == test_label)
tpr_knn5 = tpr(test_label,preds_knn5)
fpr_knn5 = fpr(test_label,preds_knn5)
# Crie um dataframe com os dados:  (1,0)
# # k: número de vizinhos usados
# # acc: acurácia obtida para cada k
# # tpr:  taxa de verdadeiros positivos  para cada k
# # fpr: taxa de falsos positivos para cada k
# O dataframe terá 2 linhas e 4 colunas.
knn_metrics = data.frame(k=11, acc_knn11,tpr_knn11,fpr_knn11, k=5, acc_knn5,tpr_knn5,fpr_knn5)
#Use cross validation com 5 folds para ajustar os hiperparâmetros especificados no script.
rpart_tune = data.frame(expand.grid(minbucket = seq(from =25, to =101, by = 25),
cp = c(0.01, 0.1),
maxdepth = 2:4,
fold = 1:3),
acc = NA)
knn_metrics
#a)Qual a configuração que apresenta a melhor performance, de acordo com a acurácia? (1,0)
for(i in 1:nrow(rpart_tune)){
fold = rpart_tune$fold[i]
split_train = train[cv_index != fold,]
split_test = train[cv_index == fold,]
m = rpart( RainTomorrow ~.,
split_train,
control = rpart.control(
minbucket = rpart_tune$minbucket[i],
maxdepth = rpart_tune$maxdepth[i],
cp = rpart_tune$cp[i]),
method='class')
ypred = predict(m, split_test, type = 'class')
rpart_tune$acc[i] = mean(ypred == split_test$RainTomorrow)
}
cv_index = create_fold_index(nrow(train),3)
#a)Qual a configuração que apresenta a melhor performance, de acordo com a acurácia? (1,0)
for(i in 1:nrow(rpart_tune)){
fold = rpart_tune$fold[i]
split_train = train[cv_index != fold,]
split_test = train[cv_index == fold,]
m = rpart( RainTomorrow ~.,
split_train,
control = rpart.control(
minbucket = rpart_tune$minbucket[i],
maxdepth = rpart_tune$maxdepth[i],
cp = rpart_tune$cp[i]),
method='class')
ypred = predict(m, split_test, type = 'class')
rpart_tune$acc[i] = mean(ypred == split_test$RainTomorrow)
}
aggregate(acc ~. ,rpart_tune, mean) %>% arrange(desc(acc)) %>% head()
m = rpart( RainTomorrow ~.,
train,
control = rpart.control(
minbucket = 25,
maxdepth = 3,
cp = 0.01),
method='class')
preds_tree = predict(m, test, type = 'class')
acc_tree = mean(preds_tree == test$RainTomorrow)
tpr_tree = tpr(test$RainTomorrow,preds_tree)
fpr_tree = fpr(test$RainTomorrow,preds_tree)
m$variable.importance
preds_logit = glm(RainTomorrow~.,train,family = "binomial")
probslog = predict(preds_logit,test,type='response')
predslog = as.numeric(probslog > 0.6)
reallog = test$RainTomorrow
table(predslog,reallog)
acc_logit = mean(predslog == reallog)
tpr_logit = tpr(test$RainTomorrow,predslog)
fpr_logit = fpr(test$RainTomorrow,predslog)
fpr_logit
tpr_logit
acc_logit
#Carregue as bibliotecas necessarias
library(tidyverse)
library(class)
library(rpart)
library(caret)
library(gbm)
tpr = function(preds, real){
sum(preds == 1 & real == 1)/sum(real == 1)
}
#falso positivo
fpr = function(preds, real){
sum(preds == 1 & real == 0)/sum(real == 0)
}
#não altere a funcao create_fold_index
create_fold_index = function(nrows, folds){
set.seed(2019)
k_index = rep(1:folds, ceiling(nrows/folds))
k_index = k_index[1:nrows]
k_index = sample(k_index)
return(k_index)
}
#Importe os dados ----
setwd("~/Disciplinas/FA084/prova01_fa084_2019_rafaella/code")
head(df)
#converta o atributo meta para fator
df$RainTomorrow = as.factor(df$RainTomorrow)
#Divida em treino e teste. Treino (70%) e teste (30%)
set.seed(2019)
rows_test = sample(nrow(df),0.3*nrow(df))
train = df[-rows_test, ]
test = df[rows_test,   ]
tr_xy = train[  , c("MinTemp","MaxTemp","Rainfall","Humidity9am","Humidity3pm")]
tst_xy = test[  ,c("MinTemp","MaxTemp","Rainfall","Humidity9am","Humidity3pm")]
tr_label =train$RainTomorrow
tst_label = test$RainTomorrow
mk5=knn(train = tr_xy,
test=tst_xy,
cl= tr_label, #fator de verdadeiras classifica??es do conjunto de treinamento
k=5)
mk5 =as.character(mk5)
acc_knn5 = sum(mk5 == tst_label)/length(tst_label)
tpr_knn5 = tpr(mk5, tst_label)
fpr_knn5 = fpr(mk5, tst_label)
mk11=knn(train = tr_xy,
test=tst_xy,
cl= tr_label,
k=11)
mk11 =as.character(mk11)
acc_knn11 = sum(mk11 == tst_label)/length(tst_label)
tpr_knn11 = tpr(mk11, tst_label)
fpr_knn11 = fpr(mk11, tst_label)
# Crie um dataframe com os dados:  (1,0)
# # k: número de vizinhos usados
# # acc: acurácia obtida para cada k
# # tpr:  taxa de verdadeiros positivos  para cada k
# # fpr: taxa de falsos positivos para cada k
# O dataframe terá 2 linhas e 4 colunas.
knn_metrics = data.frame(k=c("mk5", "mk11"),
acc=c(acc_knn5,acc_knn11),
tpr=c(tpr_knn5, tpr_knn11),
fpr= c(fpr_knn5,fpr_knn11))
knn_metrics
rpart_tune = data.frame(
expand.grid(minbucket = c(25,50,75,100),
cp = c(0.01, 0.1),
maxdepth = c(2,3,4),
fold = 1:3),
acc = NA
)
#aplicando na funcao cross validation
cv_index = create_fold_index(nrow(train),3)
for(i in 1:nrow(rpart_tune)){
fold = rpart_tune$fold[i]
split_train = train[cv_index != fold,]
split_test = train[cv_index == fold,]
m = rpart( RainTomorrow ~.,
split_train,
control = rpart.control(
minbucket = rpart_tune$minbucket[i],
maxdepth = rpart_tune$maxdepth[i],
cp = rpart_tune$cp[i]),
method='class')
ypred = predict(m, split_test, type = 'class')
rpart_tune$acc[i] = mean(ypred == split_test$RainTomorrow)
}
#vejo como ficou a acuracia em funcao da variacao do minbucket, cp e maxdepth, no conj rpart tune, pela m?dia
agr= aggregate(acc ~ minbucket + cp + maxdepth ,rpart_tune, mean) %>% arrange(desc(acc))
head(agr)
preds_tree = rpart( RainTomorrow ~.,
train,
control = rpart.control(
minbucket = 50,
maxdepth = 3,
cp = 0.01),
method='class')
#vou usar a arvore m q foi elaborada com o treino, para prever o teste
ypred = predict(preds_tree, test, type = 'class')
#quando o previsto foi igual ao real, ou seja acertou
acc_tree = mean(ypred == test$RainTomorrow)
acc_tree
tpr_tree = tpr(ypred, test$RainTomorrow)
tpr_tree
fpr_tree = fpr(ypred, test$RainTomorrow)
fpr_tree
#REGRESSAO LOGISTICA
#c)Treine um modelo de regressão logística no conjunto de treino. Se probabilidade
#de chuva for maior do que 0,6, considere que choverá. Qual a acurácia, tpr e
#fpr no conjunto de teste para este modelo? (0,5)
lrmodel = glm(formula = RainTomorrow~., train, family = 'binomial')
coef(lrmodel)
probs = predict(lrmodel,test,type='response')
preds = as.numeric(probs > 0.6)
#real no conjunto de teste
real = test$RainTomorrow
table(preds,real)
acc_logit = mean(preds==real)
acc_logit
tpr_logit = tpr(preds = preds, real = real)
tpr_t=tpr(preds, real)
tpr_logit
fpr_logit =fpr(preds = preds, real = real)
fpr_logit
gbmGrid =
preds_boost =
acc_boost =
tpr_boost =
fpr_boost =
#data frame para comparar os modelos:
modelos =data.frame(algoritimo=c("knn", "logit", "tree", "boost"),
acc= c(acc_knn5, acc_logit ,acc_tree, 'NA'),
tpr = c(tpr_knn5, tpr_logit,tpr_tree, 'NA'),
fpr = c(fpr_knn5,fpr_logit ,tpr_tree, 'NA'))
modelos
#Carregue as bibliotecas necessarias
library(tidyverse)
library(class)
library(rpart)
library(caret)
library(gbm)
#Funcoes utilizadas
#implmente as funcoes tpr e fpr
tpr = function(real,predito){
test$pred = ''
test$pred = predito
sum(test$pred==real)
test$correto = ifelse(test$pred==real,TRUE,FALSE)
}
fpr = function(real,predito){
test$pred = ''
test$pred= predito
sum(test$pred!=real)
test$errado = ifelse(test$pred!=real,TRUE,FALSE)
}
#não altere a funcao create_fold_index
create_fold_index = function(nrows, folds){
set.seed(2019)
k_index = rep(1:folds, ceiling(nrows/folds))
k_index = k_index[1:nrows]
k_index = sample(k_index)
return(k_index)
}
#Carregue as bibliotecas necessarias
library(tidyverse)
library(class)
library(rpart)
library(caret)
library(gbm)
#Funcoes utilizadas
#implmente as funcoes tpr e fpr
tpr = function(real,predito){
test$pred = ''
test$pred = predito
sum(test$pred==real)
test$correto = ifelse(test$pred==real,TRUE,FALSE)
}
fpr = function(real,predito){
test$pred = ''
test$pred= predito
sum(test$pred!=real)
test$errado = ifelse(test$pred!=real,TRUE,FALSE)
}
#não altere a funcao create_fold_index
create_fold_index = function(nrows, folds){
set.seed(2019)
k_index = rep(1:folds, ceiling(nrows/folds))
k_index = k_index[1:nrows]
k_index = sample(k_index)
return(k_index)
}
df=read.csv('./data/weather_rain_tomorrow.csv')
#converta o atributo meta para fator
df$RainTomorrow = as.factor(df$RainTomorrow)
#Divida em treino e teste
set.seed(2019)
rows_test = sample(nrow(df),ceiling(0.3*nrow(df)))
train = df[-rows_test,]
test = df[rows_test,]
train_knn = train[,c('MinTemp','MaxTemp','Rainfall','Humidity9am','Humidity3pm')]
test_knn =  test[,c('MinTemp','MaxTemp','Rainfall','Humidity9am','Humidity3pm')]
train_label = train$RainTomorrow
test_label = test$RainTomorrow
preds_knn11= knn(train = train_knn,
test = test_knn,
cl = train_label,
k=11)
preds_knn11 = as.character(preds_knn11)
acc_knn11 = sum(preds_knn11 == test_label)/length(test_label)
tpr_knn11 = tpr(real = test$RainTomorrow,predito =preds_knn11)
fpr_knn11 = fpr(real = test$RainTomorrow,predito =preds_knn11)
preds_knn5 =knn(train = train_knn,
test = test_knn,
cl = train_label,
k=5)
acc_knn5 = sum(preds_knn5 == test_label)/length(test_label)
tpr_knn5 = tpr(real = test$RainTomorrow,predito =preds_knn5)
fpr_knn5 = fpr(real = test$RainTomorrow,predito =preds_knn5)
acc_knn11=mean(acc_knn11)
acc_knn5=mean(acc_knn5)
tpr_knn11=mean(tpr_knn11)
tpr_knn5=mean(tpr_knn5)
fpr_knn11=mean(fpr_knn11)
fpr_knn5=mean(fpr_knn5)
knn_metrics=data.frame(k=c(11,5),acc=c(acc_knn11,acc_knn5),tpr=c(tpr_knn11,tpr_knn5),fpr=c(fpr_knn11,fpr_knn5))
knn_metrics
#Use cross validation com 3 folds para ajustar os hiperparâmetros especificados no script.
create_fold_index(nrow(df), 3)
rpart_tune = data.frame(test_fold=1:3, acc = NA, tpr = NA, fpr = NA)
for(i in 1:nrow(eval_lr)){
test_fold = eval_lr$test_fold[i]
rows_test = fold_index == test_fold
split_train = df[!rows_test, ]
split_test = df[rows_test, ]
fold_model =  glm(Survived~.,split_train,family = 'binomial')
ypred = predict(fold_model, split_test, type = 'response') > 0.5
ypred = as.numeric(ypred)
eval_lr$tpr[i] = tpr(ypred,split_test$Survived)
eval_lr$fpr[i] = fpr(ypred,split_test$Survived)
eval_lr$acc[i] = mean(ypred == split_test$Survived)
}
#Carregue as bibliotecas necessarias
library(tidyverse)
library(class)
library(rpart)
#install.packages('survival') # Fixa versão R 3.5 (caret)
#install.packages('caret')
library(caret)
#install.packages('gbm')
library(gbm)
#Funcoes utilizadas
#implmente as funcoes tpr e fpr
tpr = function(real, predito){
sum(predito == 1 & real == 1)/sum(real == 1)
}
fpr = function(real, predito){
sum(predito == 1 & real == 0)/sum(real == 0)
}
#não altere a funcao create_fold_index
create_fold_index = function(nrows, folds){
set.seed(2019)
k_index = rep(1:folds, ceiling(nrows/folds))
k_index = k_index[1:nrows]
k_index = sample(k_index)
return(k_index)
}
#Importe os dados ----
df = read.csv('../data/weather_rain_tomorrow.csv')
head(df)
str(df)
dim(df)
#converta o atributo meta para fator
df$RainTomorrow = as.factor(df$RainTomorrow)
#Divida em treino e teste
set.seed(2019)
rows_test = sample(nrow(df),ceiling(0.3*nrow(df)))
train = df[-rows_test,]
test = df[rows_test,]
train_knn = train[,1:5]
train_knn_label = train[,6]
test_knn = test[,1:5]
test_knn_label = test[,6]
preds_knn11= knn(train = train_knn, test = test_knn, cl = train[,6], k=11)
acc_knn11 = mean(preds_knn11 == test_knn_label)
tpr_knn11 = tpr(predito = preds_knn11, real = test_knn_label)
fpr_knn11 = fpr(predito = preds_knn11, real = test_knn_label)
preds_knn5 = knn(train = train_knn, test = test_knn, cl = train[,6], k=5)
acc_knn5 = mean(preds_knn5 == test_knn_label)
tpr_knn5 = tpr(predito = preds_knn5, real = test_knn_label)
fpr_knn5 = fpr(predito = preds_knn5, real = test_knn_label)
knn_metrics = data.frame(k = c(11,5),
acc = c(acc_knn11, acc_knn5),
tpr = c(tpr_knn11, tpr_knn5),
fpr = c(fpr_knn11, fpr_knn5))
# O dataframe terá 2 linhas e 4 colunas.
round(knn_metrics,2)
#Use cross validation com 5 folds para ajustar os hiperparâmetros especificados no script.
rpart_tune = data.frame(
expand.grid(minbucket = seq(from =25, to =100, by = 25),
cp = c(0.01, 0.1),
maxdepth = 2:4,
fold = 1:5),
acc = NA
)
cv_index =  create_fold_index(nrow(train),5)
for(i in 1:nrow(rpart_tune)){
fold = rpart_tune$fold[i]
split_train = train[cv_index != fold,]
split_test = train[cv_index == fold,]
m = rpart( RainTomorrow ~.,
split_train,
control = rpart.control(
minbucket = rpart_tune$minbucket[i],
maxdepth = rpart_tune$maxdepth[i],
cp = rpart_tune$cp[i]),
method='class')
ypred = predict(m, split_test, type = 'class')
rpart_tune$acc[i] = mean(ypred == split_test$RainTomorrow)
}
#a)Qual a configuração que apresenta a melhor performance, de acordo com a acurácia? (1,0)
winner <- aggregate(acc ~ minbucket + cp + maxdepth, rpart_tune, mean) %>% arrange(desc(acc)) %>% head()
winner
preds_tree = rpart(formula = RainTomorrow ~.,
data = train,
control = rpart.control(
minbucket=1,
maxdepth=3,
cp=0.01),
method='class')
ypred = predict(preds_tree, test, type = 'class')
acc_tree = mean(ypred == test$RainTomorrow)
tpr_tree = tpr(real = test$RainTomorrow, predito = ypred)
fpr_tree = fpr(real = test$RainTomorrow, predito = ypred)
# Importancia dos atributos
preds_tree$variable.importance
winner
preds_tree = rpart(formula = RainTomorrow ~.,
data = train,
control = rpart.control(
minbucket=1,
maxdepth=3,
cp=0.01),
method='class')
ypred = predict(preds_tree, test, type = 'class')
acc_tree = mean(ypred == test$RainTomorrow)
tpr_tree = tpr(real = test$RainTomorrow, predito = ypred)
fpr_tree = fpr(real = test$RainTomorrow, predito = ypred)
# Importancia dos atributos
preds_tree$variable.importance
preds_logit =glm(formula = RainTomorrow~., train, family = 'binomial')
probs = predict(preds_logit,test,type='response')
preds = as.numeric(probs > 0.6)
acc_logit = mean(preds==test$RainTomorrow)
tpr_logit = tpr(real = test$RainTomorrow, predito=preds)
fpr_logit = fpr(real = test$RainTomorrow, predito=preds)
coef(preds_logit)
gbmGrid = expand.grid(interaction.depth = c(1, 5, 9),
n.trees = (50:250)*50,
shrinkage = 0.1,
n.minobsinnode = 20)
train$RainTomorrow <- factor(train$RainTomorrow)
#install.packages('e1071') # Pack requerido
library(e1071)
str(train)
preds_boost =  train(RainTomorrow ~ MinTemp+MaxTemp+Rainfall+Humidity9am+Humidity3pm ,
data = train, method='gbm',
trControl = trainControl(method = 'cv',number=5),
tuneGrid = gbmGrid)
pred_ = predict(preds_boost, newdata=test, type='raw')
pred_
preds_boost
preds_boost =  train(RainTomorrow ~ MinTemp+MaxTemp+Rainfall+Humidity9am+Humidity3pm ,
data = train, method='gbm',
trControl = trainControl(method = 'cv',number=5),
tuneGrid = gbmGrid)

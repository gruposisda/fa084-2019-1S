yk2 = mk2$pred
mk3 = knn.reg(train = df_chick_train$time,test = as.data.frame(df_chick_test$time),y = df_chick_train$weight,k=3)
yk3 = mk3$pred
reg_preds = data.frame(time_test = df_chick_test$time, pred_weight2 = yk2, pred_weight3 = yk3)
reg_preds
mae2 = mean(abs(reg_preds$pred_weight2 - df_chick_test$weight))
mae3 = mean(abs(reg_preds$pred_weight3 - df_chick_test$weight))
mae2
mae3
ggplot(df_chick, aes(x = time, y= weight)) +
geom_point(size = 5)  +
geom_point(data = reg_preds, aes(x = time_test, y = pred_weight2, colour = 'K = 2'), size = 5) +
geom_point(data = reg_preds, aes(x = time_test, y = pred_weight3, colour = 'K = 3'), size = 5) +
theme_classic()
dat = read.csv('data/regression_data.csv')
dim(dat)
str(dat)
summary(dat)
set.seed(2)
rows_train=sample(x = nrow(dat), size = round(0.8*nrow(dat)), replace = F)
train = dat[rows_train,]
test = dat[-rows_train,]
head(dat)
label_col = 6 #which(colnames(dat) == 'Y')
tr_x = train[,-label_col]
tr_label = train[,label_col]
tst_x = test[,-label_col]
tst_label = test[,label_col]
head(tst_x)
head(tst_label)
#Loop com varios K
k_candidate = c(1, seq(5,20, by = 5), seq(30, 70, by =10))
k_candidate
#dataframe para avaliacao
eval_knn = data.frame(k = k_candidate,
mse_train = NA,
mse_test = NA)
eval_knn
for (i in 1:nrow(eval_knn)){
m_train = knn.reg(train = tr_x,
y = tr_label,
test = tr_x,
k = eval_knn$k[i])
m_test = knn.reg(train = tr_x,
y = tr_label,
test = tst_x,
k = eval_knn$k[i])
# para atribuir valor ? coluna do dataframe (antes sem valor)
eval_knn$mse_train[i] = mean(abs(tr_label-m_train$pred))
eval_knn$mse_test[i] = mean(abs(tst_label-m_test$pred))
print(paste0('Avaliando K =', eval_knn$k[i]))
}
eval_knn
#Visualizacao dos resultdados
plot(eval_knn$k, eval_knn$mse_test, type = 'b',
ylim = c(0,1.5), col = 'blue')
lines(eval_knn$k, eval_knn$mse_train, type = 'b',
col = 'red')
treino_x = c(1,8,0)
treino_y = c(1,1,0)
# cria gráfico do conjunto de treino usando pontos vazados
plot(treino_x, treino_y, col = c('red', 'blue', 'red'), xlim=c(0,9), ylim=c(0,9))
# adiciona os pontos do conjunto de teste utilizando pontos cheios
points(teste_x, teste_y, col = c('blue', 'red', 'blue'), pch=19)
teste_x = c(1,1,8)
teste_y = c(1,0,0)
# adiciona os pontos do conjunto de teste utilizando pontos cheios
points(teste_x, teste_y, col = c('blue', 'red', 'blue'), pch=19)
# cria gráfico do conjunto de treino usando pontos vazados
plot(treino_x, treino_y, col = c('red', 'blue', 'red'), xlim=c(0,9), ylim=c(0,9))
# adiciona os pontos do conjunto de teste utilizando pontos cheios
points(teste_x, teste_y, col = c('blue', 'red', 'blue'), pch=19)
# cria gráfico do conjunto de treino usando pontos vazados
plot(treino_x, treino_y, col = c('red', 'blue', 'red'), xlim=c(0,9), ylim=c(0,9))
# adiciona os pontos do conjunto de teste utilizando pontos cheios
points(teste_x, teste_y, col = c('blue', 'red', 'blue'), pch=19)
# cria gráfico do conjunto de treino usando pontos vazados
plot(treino_x, treino_y, col = c('red', 'blue', 'red'), xlim=c(0,9), ylim=c(0,9))
# adiciona os pontos do conjunto de teste utilizando pontos cheios
points(teste_x, teste_y, col = c('blue', 'red', 'blue'), pch=19)
# cria gráfico do conjunto de treino usando pontos vazados
plot(treino_x, treino_y, col = c('red', 'blue', 'red'), xlim=c(0,9), ylim=c(0,9))
library(FNN)
library(ggplot2)
library("generics", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.5")
detach("package:generics", unload=TRUE)
library("generics", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.5")
detach("package:generics", unload=TRUE)
setwd("~/data/fa084/02_KNN/code")
# 1.Importar dataset, dividir em treino e teste ----
setwd("~/data/fa084/02_KNN/code")
dat = read.csv('../data/dart_throws.csv')
setwd("~/data/fa084/02_KNN/code")
str(dat)
head(dat)
summary(dat)
#1.1 Comando sample
sample(x = 200, size = 10)
?sample
#1.1 Comando sample
set.seed(10)
sample(x = 200, size = 10)
sample(x = 200, size = 10)
sample(x = 200, size = 10)
sample(x = 200, size = 10)
#1.1 Comando sample
set.seed(10)
sample(x = 200, size = 10)
sample(x = 200, size = 10)
sample(x = 200, size = 10)
sample(x = 200, size = 10)
# 1.2 Dividir em treino e teste
sample(187,150)
# 1.2 Dividir em treino e teste
rows_train = sample(187,150)
# 1.2 Dividir em treino e teste
set.seed(2)
rows_train = sample(187,150)
nrow(dat)
nrow(dat)
0.8*nrow(dat)
round(0.8*nrow(dat))
rows_train = sample(nrow(dat),ceiling(0.8*nrow(dat)))
# 1.2 Dividir em treino e teste
set.seed(2)
rows_train = sample(nrow(dat),ceiling(0.8*nrow(dat)))
# 1.2 Dividir em treino e teste
set.seed(2)
rows_train = sample(nrow(dat),ceiling(0.8*nrow(dat)))
rows_train
train = dat[rows_train,]
test = dat[-rows_train,]
summary(train)
summary(test)
ggplot(train, aes(x=XCoord, y=YCoord, color=Competitor))+
geom_point(size=5) +
coord_cartesian(xlim = c(-1, 1), ylim = c(-1, 1)) +
theme_classic()
ggplot(train, aes(x=XCoord, y=YCoord, color=Competitor))+
geom_point(size=5) +
geom_point(data=test,color='black',size = 7,shape='*',stroke=7) +
coord_cartesian(xlim = c(-1, 1), ylim = c(-1, 1)) +
theme_classic()
# 2.Classificacao com KNN ----
?knn
tr_xy = train[,c('XCoord','Ycoord')]
tr_xy = train[,c('XCoord','Ycoord')]
tr_xy = train[,c('XCoord','YCoord')]
tst_xy = test[,c('XCoord','YCoord')]
head(train)
tr_xy = train[,c('XCoord','YCoord')]
tst_xy = test[,c('XCoord','YCoord')]
tr_label = train$Competitor
tst_label = test$Competitor
#Predicoes com k=1, k=3, etc.
mk1 = knn(train = tr_xy,
test = tst_xy,
cl = tr_label,
k=1)
mk1
mk1 = as.character(mk1)
mk1
tst_label
mk3
mk3 = knn(train = tr_xy,
test = tst_xy,
cl = tr_label,
k=3)
mk3 = as.character(mk3)
mk3
tst_label
#Qualidade do modelo: Acurácia de classificacao
mk1 == tst_label
#Qualidade do modelo: Acurácia de classificacao
sum(mk1 == tst_label)
#Qualidade do modelo: Acurácia de classificacao
sum(mk1 == tst_label)/37
length(tst_label)
#Qualidade do modelo: Acurácia de classificacao
sum(mk1 == tst_label)/length(tst_label)
sum(mk3 == tst_label)/length(tst_label)
#Loop para encontrar o melhor K
tst_acc = numeric()
tr_acc = numeric()
#Loop para encontrar o melhor K
tst_acc = numeric()
tr_acc = numeric()
ks = c(1,3,5,7,15,19,25)
v = numeric()
v
v = c(v,1)
v
v = c(v,1)
v
v = c(v,1)
v
v = numeric()
v = c(v,2)
v = c(v,3)
v = c(v,4)
for(k in ks){
print(paste('k é ',k))
tr_pred = knn(train = tr_xy,
test = tr_xy,
cl = tr_label,
k=k)
tst_pred = knn(train = tr_xy,
test = tst_xy,
cl = tr_label,
k=k)
tr_acc_k = sum(tr_pred == tr_label)/length(tr_label)
tst_acc_k = sum(tst_pred == tst_label)/length(tst_label)
tr_acc = c(tr_acc,tr_acc_k)
tst_acc = c(tst_acc,tst_acc_k)
}
classif_results = data.frame(k = ks,
tr_acc = tr_acc,
tst_acc = tst_acc)
classif_results
plot(classif_results$k,
classif_results$tr_acc, type = 'b',
ylim = c(0,1), col = 'blue')
plot(classif_results$k,
classif_results$tr_acc, type = 'b',
ylim = c(0,1), col = 'blue')
lines(classif_results$k,
classif_results$tst_acc, type = 'b',
ylim = c(0,1), col = 'red')
final_preds = knn(tr_xy,
tst_xy,
cl = tr_label,
k = 3)
final_preds = as.character(final_preds)
test
test$pred = ''
test$pred = final_preds
sum(test$pred==test$Competitor)
test$correto = ifelse(test$pred==test$Competitor,TRUE,FALSE)
test$pred_errado = ''
test$pred_errado[!test$correto]=as.character(test$pred)[!test$correto]
ggplot(test, aes(x=XCoord, y=YCoord, colour=Competitor,shape=correto,label=pred_errado))+
scale_shape_manual(values=c(4,1))+
geom_point(lwd=2,stroke=3)+
geom_text(fontface = 'bold',size=7,nudge_x = 0.1)+
coord_cartesian(xlim = c(-1, 1), ylim = c(-1, 1)) +
theme_classic()
# 3.Regressão com KNN ----
#Visualização
df_chick = read.csv('../data/chickweight.csv')
rm(list=ls())
# 3.Regressão com KNN ----
#Visualização
df_chick = read.csv('../data/chickweight.csv')
id_test = c(3,6,9,11)
df_chick_train = df_chick[-id_test,]
df_chick_test = df_chick[id_test,]
mk2 = knn.reg(train = df_chick_train$time,test = as.data.frame(df_chick_test$time),y = df_chick_train$weight,k=2)
yk2 = mk2$pred
mk3 = knn.reg(train = df_chick_train$time,test = as.data.frame(df_chick_test$time),y = df_chick_train$weight,k=3)
yk3 = mk3$pred
reg_preds = data.frame(time_test = df_chick_test$time, pred_weight2 = yk2, pred_weight3 = yk3)
reg_preds
mae2 = mean(abs(reg_preds$pred_weight2 - df_chick_test$weight))
mae3 = mean(abs(reg_preds$pred_weight3 - df_chick_test$weight))
mae2
mae3
ggplot(df_chick, aes(x = time, y= weight)) +
geom_point(size = 5)  +
geom_point(data = reg_preds, aes(x = time_test, y = pred_weight2, colour = 'K = 2'), size = 5) +
geom_point(data = reg_preds, aes(x = time_test, y = pred_weight3, colour = 'K = 3'), size = 5) +
theme_classic()
# Regressão com mais de uma variavel preditora
#importar conjunto de dados
dat = read.csv('../data/regression_data.csv')
str(dat)
head(dat)
head(dat)
summary(dat)
# 1.2 Dividir em treino e teste
set.seed(2)
rows_train = sample(nrow(dat),ceiling(0.8*nrow(dat)))
train = dat[rows_train,]
test = dat[-rows_train,]
head(dat)
colnames(dat)
colnames(dat) == 'Y'
which(colnames(dat) == 'Y')
pos_label = which(colnames(dat) == 'Y')
tr_x = train[,-pos_label]
tst_x = test[,pos_label]
#Loop com varios K
?seq
seq(1,10)
seq(1,100)
seq(1,100,2)
seq(1,100,3)
c(1,3,seq(5,20,5))
c(1,3,seq(5,20,5),seq(30,70,10))
ks = c(1,3,seq(5,20,5),seq(30,70,10))
#dataframe para avaliacao
reg_results = data.frame(k = ks,
mae_tr = NA,
mae_tst = NA)
reg_results
?knn.reg
reg_results$k
reg_results$k[1]
reg_results$k[2]
reg_results$k[3]
reg_results$k[4]
reg_results
for(i in 1:nrow(reg_results)){
print(i)
m_train = knn.reg(train = tr_x,
test = tr_x,
y = tr_label,
k = reg_results$k[i])
m_test = knn.reg(train = tr_x,
test = tst_x,
y = tr_label,
k = reg_results$k[i])
reg_results$mae_tr[i] = mean(abs(tr_label-m_train))
reg_results$mae_tst[i] mean(abs(tst_label-m_test))
}
for(i in 1:nrow(reg_results)){
print(i)
m_train = knn.reg(train = tr_x,
test = tr_x,
y = tr_label,
k = reg_results$k[i])
m_test = knn.reg(train = tr_x,
test = tst_x,
y = tr_label,
k = reg_results$k[i])
reg_results$mae_tr[i] = mean(abs(tr_label-m_train))
reg_results$mae_tst[i] mean(abs(tst_label-m_test))
}
for(i in 1:nrow(reg_results)){
print(i)
m_train = knn.reg(train = tr_x,
test = tr_x,
y = tr_label,
k = reg_results$k[i])
m_test = knn.reg(train = tr_x,
test = tst_x,
y = tr_label,
k = reg_results$k[i])
reg_results$mae_tr[i] = mean(abs(tr_label-m_train))
reg_results$mae_tst[i] = mean(abs(tst_label-m_test))
}
pos_label = which(colnames(dat) == 'Y')
tr_x = train[,-pos_label]
tst_x = test[,-pos_label]
tr_label = train[,pos_label]
tst_label = test[,pos_label]
#Loop com varios K
?seq
ks = c(1,3,seq(5,20,5),seq(30,70,10))
#dataframe para avaliacao
reg_results = data.frame(k = ks,
mae_tr = NA,
mae_tst = NA)
reg_results
for(i in 1:nrow(reg_results)){
print(i)
m_train = knn.reg(train = tr_x,
test = tr_x,
y = tr_label,
k = reg_results$k[i])
m_test = knn.reg(train = tr_x,
test = tst_x,
y = tr_label,
k = reg_results$k[i])
reg_results$mae_tr[i] = mean(abs(tr_label-m_train))
reg_results$mae_tst[i] = mean(abs(tst_label-m_test))
}
for(i in 1:nrow(reg_results)){
print(i)
m_train = knn.reg(train = tr_x,
test = tr_x,
y = tr_label,
k = reg_results$k[i])
m_test = knn.reg(train = tr_x,
test = tst_x,
y = tr_label,
k = reg_results$k[i])
reg_results$mae_tr[i] = mean(abs(tr_label-m_train$pred))
reg_results$mae_tst[i] = mean(abs(tst_label-m_test$pred))
}
reg_results
m_test
m_test$call
m_test$pred
m_test$k
m_test$n
m_test$residuals
reg_results
#Visualizacao dos resultdados
plot(reg_results$k,
reg_results$mae_tr, type = 'b', col = 'blue')
lines(reg_results$k,
reg_results$mae_tst, type = 'b', col = 'red')
#Visualizacao dos resultdados
plot(reg_results$k,
reg_results$mae_tr, type = 'b', col = 'blue')
lines(reg_results$k,
reg_results$mae_tst, type = 'b', col = 'red')
#Visualizacao dos resultdados
plot(reg_results$k,
reg_results$mae_tst, type = 'b', col = 'blue')
lines(reg_results$k,
reg_results$mae_tr, type = 'b', col = 'red')
#Visualizacao dos resultdados
plot(reg_results$k,
reg_results$mae_tr, type = 'b',
ylim = c(0,1.5), col = 'blue')
lines(reg_results$k,
reg_results$mae_tst, type = 'b',
ylim = c(0,1.5), col = 'red')
source('~/data/fa084/02_KNN/code/02_KNN_class.R')
setwd("~/repos/fa084/03_linear_regression/pre")
dat = read.csv('fa084_cana_estagio.csv')
dat
plot(dat$estagio,dat$tch)
train = dat[1:500,]
tst = dat[-(1:500),]
train = dat[1:600,]
tst = dat[-(1:600),]
model = lm(TCH~poly(Estagio,3))
model = lm(TCH~poly(Estagio,3),data = dat)
dat
head(dat)
dat = read.csv('fa084_cana_estagio.csv')
train = dat[1:600,]
tst = dat[-(1:600),]
model = lm(TCH~poly(Estagio,3),data = dat)
dat
model = lm(TCH~poly(Estagio,3),data = dat)
predict(model, test)
predict(model,newdata = tst)
m1 = predict(model,newdata = tst)
model = lm(TCH~poly(Estagio,10),data = dat)
model = lm(TCH~poly(Estagio,9),data = dat)
model = lm(TCH~poly(Estagio,5),data = dat)
model = lm(TCH~poly(Estagio,7),data = dat)
model = lm(TCH~poly(Estagio,6),data = dat)
model = lm(TCH~poly(Estagio,5),data = dat)
model = lm(TCH~poly(Estagio,1),data = dat)
model = lm(TCH~poly(Estagio,2),data = dat)
model = lm(TCH~poly(Estagio,4),data = dat)
modelA = lm(TCH~poly(Estagio,3),data = dat)
modelB = lm(TCH~poly(Estagio,4),data = dat)
pA = predict(modelA,newdata = tst)
pB = predict(modelB,newdata = tst)
maeA = mean(abs(pA = tst$TCH))
maeA = mean(abs(pA - tst$TCH))
maeB = mean(abs(pB - tst$TCH))
plot(dat$estagio,dat$tch)
plot(dat$Estagio,dat$TCH)
abline(modelA)
modelA
plot(modelA)
plot(modelA)
modelA$df.residual
modelA$residuals
modelA = lm(TCH~poly(Estagio,1),data = dat)
modelB = lm(TCH~poly(Estagio,5),data = dat)
pA = predict(modelA,newdata = tst)
pB = predict(modelB,newdata = tst)
maeA = mean(abs(pA - tst$TCH))
maeB = mean(abs(pB - tst$TCH))
maeA
maeB
# Carregue o conjunto de dados
# Lembre-se de definir o diretorio corretamente
RA=123456
df = read.csv('../pre/data/fa084_cana_estagio.csv')
rows_train = sample(nrow(df), 0.75*nrow(df))
# Carregue o conjunto de dados
# Lembre-se de definir o diretorio corretamente
RA=123456
df = read.csv('../pre/data/fa084_cana_estagio.csv')
#divida em treino e teste:
set.seed(RA)
rows_train = sample(nrow(df), 0.75*nrow(df))
train = df[rows_train,]
test = df[-rows_train,]
#Substitua essa linha pelo sample que cria a variavel a
a = sample(c(1,2,3),1)
#Crie as variaveis a e b, que representam o grau do polinomio a ser treinado em cada caso
set.seed(RA)
#Substitua essa linha pelo sample que cria a variavel a
a = sample(c(1,2,3),1)
set.seed(RA)
b = sample(c(4,5),1)
#treine os dois modelos com a funcao lm() no conjunto de treino.
#Dica: A formula é TCH~poly(Estagio,grau) onde grau é o grau do polinomio.
#Devemos ter um modelo com grau a e outro com grau b
modelo_grau_a = lm(TCH~Estagio,data = df)
#treine os dois modelos com a funcao lm() no conjunto de treino.
#Dica: A formula é TCH~poly(Estagio,grau) onde grau é o grau do polinomio.
#Devemos ter um modelo com grau a e outro com grau b
modelo_grau_a = lm(TCH~poly(Estagio,a),data = df)
modelo_grau_b = lm(TCH~poly(Estagio,b),data = df)
#Faca a predicao no conjunto de teste usando o modelo a e o modelo b.
#Use a funcao predict("modelo para fazer a previsao", newdata = "conjunto de dados de teste").
pred_a = predict(modelo_grau_a,newdata = test)
pred_b = predict(modelo_grau_b,newdata = test)
#Calcule o erro  medio absoluto (mae) em relacao a TCH do conjunto de teste para cada modelo.
mean(abs(pred_a - test$TCH))
mean(abs(pred_a - test$TCH))
mean(abs(pred_b - test$TCH))
